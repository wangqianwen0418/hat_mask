import keras
from keras.models import Model, Sequential, load_model
import keras.layers as layers
from keras.datasets import cifar10
from keras.preprocessing.image import ImageDataGenerator
from keras.regularizers import l2
from keras import backend as K

from keras.applications.vgg16 import VGG16
import ulti
from ulti import load_data
import os

# parameter
ulti.data_dir = "data"
ulti.num_imgs = 500
ulti.ratio_train = 0.8
ulti.w = 224
ulti.h = 224

batch_size = 16
epochs = 300
save_dir = "save_model"
model_path = os.path.join(save_dir, "model.h5")
tflog_dir = os.path.join(save_dir, "tf_log")

x_train, y_train = load_data(dset="train", target="no_hat")
x_test, y_test = load_data(dset="test", target="no_hat")

# model

# # model like vgg
base_model =  VGG16(weights=None, include_top=False, pooling=None, input_shape=(224,224,3))
inputs = base_model.input
# x = base_model.output
x = base_model.get_layer("block3_pool").output
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(100, activation='relu', name='fc1')(x)
x = layers.Dense(1, activation="sigmoid", name="fc2")(x)


model = Model(inputs=inputs,outputs=x)

# train
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

# This will do preprocessing and realtime data augmentation:
datagen = ImageDataGenerator(
    featurewise_center=False,  # set input mean to 0 over the dataset
    samplewise_center=False,  # set each sample mean to 0
    featurewise_std_normalization=False,  # divide inputs by std of the dataset
    samplewise_std_normalization=False,  # divide each input by its std
    zca_whitening=False,  # apply ZCA whitening
    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)
    width_shift_range=0.,  # randomly shift images horizontally (fraction of total width)
    height_shift_range=0.,  # randomly shift images vertically (fraction of total height)
    horizontal_flip=True,  # randomly flip images
    vertical_flip=False)  # randomly flip images

datagen.fit(x_train)
checkpoint = keras.callbacks.ModelCheckpoint(filepath=model_path, verbose=1, save_best_only=True, save_weights_only=False)
tf_log = keras.callbacks.TensorBoard(log_dir=tflog_dir, batch_size=batch_size)
callbacks = [checkpoint, tf_log]

# Fit the model on the batches generated by datagen.flow().
model.fit_generator(datagen.flow(x_train, y_train,
                                    batch_size=batch_size),
                    steps_per_epoch=x_train.shape[0] // batch_size,
                    epochs=epochs,
                    validation_data=(x_test, y_test),
                    verbose=2,
                    callbacks=callbacks)


# Evaluate model with test data set and share sample prediction results
evaluation = model.evaluate_generator(datagen.flow(x_test, y_test,
                                      batch_size=batch_size),
                                      steps=x_test.shape[0] // batch_size)

print('Model Accuracy = %.2f' % (evaluation[1]))

# model.fit(x_train, y_train,
#           batch_size=batch_size,
#           epochs=epochs,
#           validation_data=(x_test, y_test))

# # evaluate
# score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)
# print('Test score:', score)
# print('Test accuracy:', acc)